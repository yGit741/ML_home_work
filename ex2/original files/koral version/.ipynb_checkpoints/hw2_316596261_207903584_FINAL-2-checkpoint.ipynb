{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e-bl7io3e6T",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6bd0516e7cb654f5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 2: Decision Trees\n",
    "\n",
    "In this assignment you will implement a Decision Tree algorithm as learned in class.\n",
    "\n",
    "## Do not start the exercise until you fully understand the submission guidelines.\n",
    "\n",
    "* The homework assignments are executed automatically. \n",
    "* Failure to comply with the following instructions will result in a significant penalty. \n",
    "* Appeals regarding your failure to read these instructions will be denied. \n",
    "* Kindly reminder: the homework assignments contribute 50% of the final grade.\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This Jupyter notebook contains all the step-by-step instructions needed for this exercise.\n",
    "1. Write **efficient**, **vectorized** code whenever possible. Some calculations in this exercise may take several minutes when implemented efficiently, and might take much longer otherwise. Unnecessary loops will result in point deductions.\n",
    "1. You are responsible for the correctness of your code and should add as many tests as you see fit to this jupyter notebook. Tests will not be graded nor checked.\n",
    "1. Complete the required functions in `hw2.py` script only. This exercise is graded automatically, and only the `hw2.py` script is tested.\n",
    "1. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/), numpy and pandas only. **Do not import anything else.**\n",
    "1. Your code must run without errors. Use at least `numpy` 1.15.4. Any code that cannot run will not be graded.\n",
    "1. Write your own code. Cheating will not be tolerated.\n",
    "1. Submission includes a zip file that contains the `hw2.py` script as well as this notebook, with your ID as the file name. For example, `hw2_123456789_987654321.zip` if you submitted in pairs and `hw2_123456789.zip` if you submitted the exercise alone. \n",
    "\n",
    "Please use only a **zip** file in your submission.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## Please sign that you have read and understood the instructions: \n",
    "\n",
    "### *** 316596261, 207903584 ***\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYQ_8bw_3e6X"
   },
   "source": [
    "# I have read and understood the instructions: *** YOUR ID HERE ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vut2iIsz3e6Y",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ed9fe7b1026e33cb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "84f07363-2433-44eb-9a3b-f17f30d35e58"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make the notebook automatically reload external python modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Mw0xzv43e6a",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c6ac605270c2b091",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Warmup - OOP in python\n",
    "\n",
    "Our desicion tree will be implemented using a dedicated python class. Python classes are very similar to classes in other object oriented programming languages you might be familiar with.\n",
    "\n",
    "\n",
    "You can use the following [site](https://jeffknupp.com/blog/2014/06/18/improve-your-python-python-classes-and-object-oriented-programming/) to learn about classes in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Qgbizw5z3e6a"
   },
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.children = []\n",
    "\n",
    "    def add_child(self, node):\n",
    "        self.children.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NLVWsCkE3e6b",
    "outputId": "90782402-303b-4f79-c812-325dd58d2233"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Node at 0x2290dff4910>, <__main__.Node at 0x2290dff4550>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = Node(5)\n",
    "p = Node(6)\n",
    "q = Node(7)\n",
    "n.add_child(p)\n",
    "n.add_child(q)\n",
    "n.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tqwHyyQr3wRh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Chi square table values ###\n",
    "# The first key is the degree of freedom \n",
    "# The second key is the p-value cut-off\n",
    "# The values are the chi-statistic that you need to use in the pruning\n",
    "\n",
    "chi_table = {1: {0.5 : 0.45,\n",
    "             0.25 : 1.32,\n",
    "             0.1 : 2.71,\n",
    "             0.05 : 3.84,\n",
    "             0.0001 : 100000},\n",
    "         2: {0.5 : 1.39,\n",
    "             0.25 : 2.77,\n",
    "             0.1 : 4.60,\n",
    "             0.05 : 5.99,\n",
    "             0.0001 : 100000},\n",
    "         3: {0.5 : 2.37,\n",
    "             0.25 : 4.11,\n",
    "             0.1 : 6.25,\n",
    "             0.05 : 7.82,\n",
    "             0.0001 : 100000},\n",
    "         4: {0.5 : 3.36,\n",
    "             0.25 : 5.38,\n",
    "             0.1 : 7.78,\n",
    "             0.05 : 9.49,\n",
    "             0.0001 : 100000},\n",
    "         5: {0.5 : 4.35,\n",
    "             0.25 : 6.63,\n",
    "             0.1 : 9.24,\n",
    "             0.05 : 11.07,\n",
    "             0.0001 : 100000},\n",
    "         6: {0.5 : 5.35,\n",
    "             0.25 : 7.84,\n",
    "             0.1 : 10.64,\n",
    "             0.05 : 12.59,\n",
    "             0.0001 : 100000},\n",
    "         7: {0.5 : 6.35,\n",
    "             0.25 : 9.04,\n",
    "             0.1 : 12.01,\n",
    "             0.05 : 14.07,\n",
    "             0.0001 : 100000},\n",
    "         8: {0.5 : 7.34,\n",
    "             0.25 : 10.22,\n",
    "             0.1 : 13.36,\n",
    "             0.05 : 15.51,\n",
    "             0.0001 : 100000},\n",
    "         9: {0.5 : 8.34,\n",
    "             0.25 : 11.39,\n",
    "             0.1 : 14.68,\n",
    "             0.05 : 16.92,\n",
    "             0.0001 : 100000},\n",
    "         10: {0.5 : 9.34,\n",
    "              0.25 : 12.55,\n",
    "              0.1 : 15.99,\n",
    "              0.05 : 18.31,\n",
    "              0.0001 : 100000},\n",
    "         11: {0.5 : 10.34,\n",
    "              0.25 : 13.7,\n",
    "              0.1 : 17.27,\n",
    "              0.05 : 19.68,\n",
    "              0.0001 : 100000}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZRL1wxp3e6b",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2f1ceb251c649b62",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Data preprocessing\n",
    "\n",
    "For the following exercise, we will use a dataset containing mushroom data `agaricus-lepiota.csv`. \n",
    "\n",
    "This data set includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous\n",
    "one (=there are only two classes **edible** and **poisonous**). \n",
    "    \n",
    "The dataset contains 8124 observations with 21 features and the class:\n",
    "1. cap-shape: bell=b,conical=c,convex=x,flat=f,knobbed=k,sunken=s\n",
    "1. cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s\n",
    "1. cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y\n",
    "1. bruises: bruises=t,no=f\n",
    "1. odor: almond=a,anise=l,creosote=c,fishy=y,foul=f, musty=m,none=n,pungent=p,spicy=s\n",
    "1. gill-attachment: attached=a,descending=d,free=f,notched=n\n",
    "1. gill-spacing: close=c,crowded=w,distant=d\n",
    "1. gill-size: broad=b,narrow=n\n",
    "1. gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g,green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y\n",
    "1. stalk-shape: enlarging=e,tapering=t\n",
    "1. stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s\n",
    "1. stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s\n",
    "1. stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n",
    "1. stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n",
    "1. veil-type: partial=p,universal=u\n",
    "1. veil-color: brown=n,orange=o,white=w,yellow=y\n",
    "1. ring-number: none=n,one=o,two=t\n",
    "1. ring-type: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z\n",
    "1. spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y\n",
    "1. population: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y\n",
    "1. habitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d\n",
    "\n",
    "First, we will read and explore the data using pandas and the `.read_csv` method. Pandas is an open source library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525
    },
    "id": "xAgB8Cup3e6c",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d79cb4542926ad3f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "89aa5cca-a4bf-423a-c178-978df23d2275"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'agaricus-lepiota.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43magaricus-lepiota.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m data\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'agaricus-lepiota.csv'"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv('agaricus-lepiota.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cH0cvId3e6c"
   },
   "source": [
    "One of the advantages of the Decision Tree algorithm is that almost no preprocessing is required. However, finding missing values is always required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNQa1joR3e6d"
   },
   "outputs": [],
   "source": [
    "data = data.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WwXlxCF3e6d"
   },
   "source": [
    "We will split the dataset to `training` and `test` sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X9CWT0w43e6d",
    "outputId": "31eaff1b-7d55-48e7-83d2-e3d123229bb7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Making sure the last column will hold the labels\n",
    "X, y = data.drop('class', axis=1), data['class']\n",
    "X = np.column_stack([X,y])\n",
    "# split dataset using random_state to get the same split each time\n",
    "X_train, X_test = train_test_split(X, random_state=99)\n",
    "\n",
    "print(\"Training dataset shape: \", X_train.shape)\n",
    "print(\"Testing dataset shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQX6CR573e6d",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fd7b0191f3f1e897",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Impurity Measures (10 points)\n",
    "\n",
    "Impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. Implement the functions `calc_gini` and `calc_entropy` in `hw2.py`. You are encouraged to test your implementation according to the expected behavior of those measures as seen in class. (5 points each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83Vg88eJ3e6e"
   },
   "outputs": [],
   "source": [
    "#from hw2 import calc_gini, calc_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgAUx8Gb30dY"
   },
   "outputs": [],
   "source": [
    "def calc_gini(data):\n",
    "    \"\"\"\n",
    "    Calculate gini impurity measure of a dataset.\n",
    " \n",
    "    Input:\n",
    "    - data: any dataset where the last column holds the labels.\n",
    " \n",
    "    Returns:\n",
    "    - gini: The gini impurity value.\n",
    "    \"\"\"\n",
    "    gini = 0.0\n",
    "    label = data[: , -1]\n",
    "    unique, counts = np.unique(label, return_counts=True)\n",
    "    total = counts.sum()\n",
    "   \n",
    "    gini = 1 - pow((counts/total), 2).sum()\n",
    "    pass\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return gini\n",
    "\n",
    "def calc_entropy(data):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a dataset.\n",
    "\n",
    "    Input:\n",
    "    - data: any dataset where the last column holds the labels.\n",
    "\n",
    "    Returns:\n",
    "    - entropy: The entropy value.\n",
    "    \"\"\"\n",
    "    entropy = 0.0\n",
    "    label = data[: , -1]\n",
    "    unique, counts = np.unique(label, return_counts=True)\n",
    "    total  = counts.sum()\n",
    "    p_arr  = counts/total\n",
    "\n",
    "    entropy = -((p_arr * np.log2(p_arr)).sum())\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    pass\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-v2fGRFE3e6e",
    "outputId": "9c1e9056-5130-4faf-b519-9bdaddecba8f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### Your tests here #####\n",
    "\n",
    "calc_gini(X), calc_entropy(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTwhvfQC3e6e"
   },
   "source": [
    "## Goodness of Split (10 Points)\n",
    "\n",
    "Given some feature, the Goodnees of Split measures the reduction in the impurity if we split the data according to the feature.\n",
    "$$\n",
    "\\Delta\\varphi(S, A) = \\varphi(S) - \\sum_{v\\in Values(A)} \\frac{|S_v|}{|S|}\\varphi(S_v)\n",
    "$$\n",
    "\n",
    "In our implementation the goodness_of_split function will return either the Goodness of Split or the Gain Ratio as learned in class. You'll control the return value with the `gain_ratio` parameter. If this parameter will set to False (the default value) it will return the regular Goodness of Split. If it will set to True it will return the Gain Ratio.\n",
    "$$\n",
    "GainRatio(S,A)=\\frac{InformationGain(S,A)}{SplitInformation(S,A)}\n",
    "$$\n",
    "Where:\n",
    "$$\n",
    "InformationGain(S,A)=Goodness\\ of\\ Split\\ calculated\\ with\\ Entropy\\ as\\ the\\ Impurity\\ function \\\\\n",
    "SplitInformation(S,A)=- \\sum_{a\\in A} \\frac{|S_a|}{|S|}\\log\\frac{|S_a|}{|S|}\n",
    "$$\n",
    "\n",
    "Implement the function `goodness_of_split` in `hw2.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o36uxEA23e6f"
   },
   "outputs": [],
   "source": [
    "#from hw2 import goodness_of_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jbn7Wem335QJ"
   },
   "outputs": [],
   "source": [
    "def goodness_of_split(data, feature, impurity_func, gain_ratio=False):\n",
    "    \"\"\"\n",
    "    Calculate the goodness of split of a dataset given a feature and impurity function.\n",
    "    Note: Python support passing a function as arguments to another function\n",
    "    Input:\n",
    "    - data: any dataset where the last column holds the labels.\n",
    "    - feature: the feature index the split is being evaluated according to.\n",
    "    - impurity_func: a function that calculates the impurity.\n",
    "    - gain_ratio: goodness of split or gain ratio flag.\n",
    "\n",
    "    Returns:\n",
    "    - goodness: the goodness of split value\n",
    "    - groups: a dictionary holding the data after splitting \n",
    "              according to the feature values.\n",
    "    \"\"\"\n",
    "    goodness = 0\n",
    "    groups = {}\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    dataset_size = data.shape[0] # size of the dataset\n",
    "    unique = np.unique(data[:,feature]).tolist()\n",
    "   \n",
    "    sum_impurities = 0  # calculate the weighted sum of impurities for each group of data\n",
    "    for value in unique:\n",
    "        i = data[:,feature] == value\n",
    "        sum_impurities = sum_impurities + (i.sum() / dataset_size) * impurity_func(data[i])\n",
    "        groups[value] = data[i]\n",
    "\n",
    "    info_gain = impurity_func(data) - sum_impurities  # calculate the information gain\n",
    "\n",
    "    if gain_ratio:\n",
    "        split_info = 0 # calculate the split of information\n",
    "        for value in unique:\n",
    "            prop = (data[:,feature] == value).sum() / dataset_size\n",
    "            split_info = split_info + (prop * np.log(prop))\n",
    "        split_info = -split_info\n",
    "        if split_info == 0:\n",
    "            print(f'bad soi: unique values {unique}')\n",
    "            raise(Exception())\n",
    "        goodness = info_gain / split_info\n",
    "    else:\n",
    "        goodness = info_gain\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return goodness, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X8nhvTu-3e6f",
    "outputId": "73c7cc27-7cbc-4901-c1ef-2271dea6922e"
   },
   "outputs": [],
   "source": [
    "##### Your tests here #####\n",
    "\n",
    "# python support passing a function as arguments to another function.\n",
    "\n",
    "goodness_gini, split_values_gini = goodness_of_split(X, 0, calc_gini)\n",
    "goodness_entropy, split_values_entropy = goodness_of_split(X, 0, calc_entropy)\n",
    "\n",
    "goodness_gini, goodness_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1f36PCx43e6f"
   },
   "source": [
    "## Building a Decision Tree (35 points)\n",
    "\n",
    "Implement the class `DecisionNode` in `hw2.py`.\n",
    "\n",
    "Use a Python class to construct the decision tree. Your class should support the following functionality:\n",
    "\n",
    "1. Initiating a node for a decision tree. You will need to use several class methods and class attributes that appear in `hw2.py`. \n",
    "1. Note the following attributes and methods for each node:\n",
    "    1. `self.data` holds the relevant data to split that node (ndarray).\n",
    "    1. `self.feature` holds the best feature that splits the node (int).\n",
    "    1. `self.pred` holds the prediction of the entire node (string).\n",
    "    1. `self.depth` holds the depth of the node (int).\n",
    "    1. `self.children` holds the objects of the children of the node (list).\n",
    "    1. `self.children_values` holds the value of the feature associated with the children (list).\n",
    "    1. `self.terminal` determines if the node is a leaf (boolean).\n",
    "    1. `self.chi` holds the chi square value (int).\n",
    "    1. `self.max_depth` holds the maximum allowed depth of the entire tree (int).\n",
    "    1. `self.gain_ratio` determines if gain_ratio is used (boolean).\n",
    "\n",
    "1. Your code should support both Gini and Entropy as impurity measures. \n",
    "1. The provided data includes categorical data. In this exercise, when splitting a node create the number of children needed according to the attribute unique values.\n",
    "1. Complete the class `DecisionNode`. Implementation details are up to you, but maintain the function signature and outputs. Make sure you are not changing the provided functions / variables we provided.\n",
    "1. You can create auxiliary functions, methods and variables.\n",
    "1. Complete the function `build_tree`. This function should get the training dataset and the impurity as inputs, initiate a root for the decision tree and construct the tree according to the procedure you learned in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lMNjHq63e6f"
   },
   "outputs": [],
   "source": [
    "#from hw2 import build_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7b6qdyQy4E8r"
   },
   "outputs": [],
   "source": [
    "class DecisionNode:\n",
    "\n",
    "    def __init__(self, data, feature=-1,depth=0, chi=1, max_depth=1000, gain_ratio=False):\n",
    "        \n",
    "        self.data = data # the relevant data for the node\n",
    "        self.labels, self.counts = np.unique(self.data[:,-1], return_counts=True)\n",
    "        self.feature = feature # column index of criteria being tested\n",
    "        self.pred = self.calc_node_pred() # the prediction of the node\n",
    "        self.depth = depth # the current depth of the node\n",
    "        self.children = [] # array that holds this nodes children\n",
    "        self.children_values = []\n",
    "        self.terminal = False # determines if the node is a leaf\n",
    "        self.chi = chi \n",
    "        self.max_depth = max_depth # the maximum allowed depth of the tree\n",
    "        self.gain_ratio = gain_ratio \n",
    "        self.n_features_used = self.data.shape[1]-1\n",
    "        self.goodness = -1\n",
    "        self.chi_sqr = None\n",
    "    \n",
    "    def update_chi_sqr_value(self):\n",
    "        \"\"\"\n",
    "        Calculate the chi square value of the node.\n",
    "        This method assumes there are two labels!\n",
    "        \n",
    "        Returns:\n",
    "        - chi_sqr: the chi square value of the node.\n",
    "        \"\"\"\n",
    "        chi_sqr = 0\n",
    "        py0 = self.counts[0] / self.data.shape[0]\n",
    "        py1 = self.counts[1] / self.data.shape[0]\n",
    "        vals = np.unique(self.data[:,self.feature]).tolist()\n",
    "        for f in vals:\n",
    "            df = (self.data[:,self.feature] == f).sum()\n",
    "            pf = ((self.data[:,self.feature] == f) & (self.data[:,-1] == self.labels[0])).sum()\n",
    "            nf = ((self.data[:,self.feature] == f) & (self.data[:,-1] == self.labels[1])).sum()\n",
    "            e0 = df*py0\n",
    "            e1 = df*py1\n",
    "            chi_sqr = chi_sqr + (np.square(pf - e0) / e0) + (np.square(nf - e1) / e1)\n",
    "        \n",
    "        self.chi_sqr = chi_sqr\n",
    "        \n",
    "        \n",
    "    def calc_node_pred(self):\n",
    "        \"\"\"\n",
    "        Calculate the node prediction.\n",
    "\n",
    "        Returns:\n",
    "        - pred: the prediction of the node\n",
    "        \"\"\"\n",
    "        pred = None\n",
    "        ###########################################################################\n",
    "        # TODO: Implement the function.                                           #\n",
    "        ###########################################################################\n",
    "        label = self.data[: , -1]\n",
    "        unique, counts = np.unique(label, return_counts=True)\n",
    "        pred = unique[np.where(counts==max(counts))[0][0]] # most common label for each node\n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "        return pred\n",
    "        \n",
    "    def add_child(self, node, val):\n",
    "        \"\"\"\n",
    "        Adds a child node to self.children and updates self.children_values\n",
    "\n",
    "        This function has no return value\n",
    "        \"\"\"\n",
    "        self.children.append(node)\n",
    "        self.children_values.append(val)\n",
    "     \n",
    "    def split(self, impurity_func):\n",
    "\n",
    "        \"\"\"\n",
    "        Splits the current node according to the impurity_func. This function finds\n",
    "        the best feature to split according to and create the corresponding children.\n",
    "        This function should support pruning according to chi and max_depth.\n",
    "\n",
    "        Input:\n",
    "        - The impurity function that should be used as the splitting criteria\n",
    "\n",
    "        This function has no return value\n",
    "        \"\"\"\n",
    "        ###########################################################################\n",
    "        # TODO: Implement the function.                                           #\n",
    "        ###########################################################################\n",
    "        if len(self.labels) == 1:\n",
    "            self.terminal = True\n",
    "            return            \n",
    "        if self.depth >= self.max_depth:\n",
    "            self.terminal = True\n",
    "            return\n",
    "        if self.n_features_used == 0:\n",
    "            print(f'No features: depth {self.depth} data_size is {self.data.shape}')\n",
    "            self.terminal = True\n",
    "            return\n",
    "        \n",
    "        # Select the attribute (feature) with the best goodness_of_split #\n",
    "        best_split_values = None\n",
    "        for f in range(self.n_features_used):\n",
    "            # only check features with more then one unique values \n",
    "            # ====================================================\n",
    "            if np.unique(self.data[:,f]).shape[0] <= 1:\n",
    "                continue\n",
    "            goodness, split_values = goodness_of_split(self.data, f, impurity_func, gain_ratio=self.gain_ratio)\n",
    "            if goodness > self.goodness:\n",
    "                self.goodness = goodness\n",
    "                best_split_values = split_values\n",
    "                self.feature = f\n",
    "        \n",
    "        if len(best_split_values) > 1:\n",
    "            # Update the node chi_sqr once we have selected the feature #\n",
    "            #############################################################\n",
    "            self.update_chi_sqr_value()\n",
    "\n",
    "            # Init and updated children nodes (if needed) #\n",
    "            ###############################################\n",
    "            df = len(best_split_values) - 1\n",
    "            if (self.chi == 1) or (self.chi_sqr >= chi_table[df][self.chi]):\n",
    "                for val, sub_data in best_split_values.items():\n",
    "                    child_node = DecisionNode(sub_data, feature=-1,depth=self.depth+1, chi=self.chi, max_depth=self.max_depth, gain_ratio=self.gain_ratio)\n",
    "                    self.add_child(child_node, val)\n",
    "            else:\n",
    "                self.terminal = True\n",
    "        else:\n",
    "            print(f'No more possible splits: num of features: {self.n_features_used} data_size is {self.data.shape}')\n",
    "            self.terminal = True\n",
    "\n",
    "        \n",
    "        ###########################################################################\n",
    "        #                             END OF YOUR CODE                            #\n",
    "        ###########################################################################\n",
    "\n",
    "def build_tree(data, impurity, gain_ratio=False, chi=1, max_depth=1000):\n",
    "    \"\"\"\n",
    "    Build a tree using the given impurity measure and training dataset. \n",
    "    You are required to fully grow the tree until all leaves are pure unless\n",
    "    you are using pruning\n",
    "\n",
    "    Input:\n",
    "    - data: the training dataset.\n",
    "    - impurity: the chosen impurity measure. Notice that you can send a function\n",
    "                as an argument in python.\n",
    "    - gain_ratio: goodness of split or gain ratio flag\n",
    "\n",
    "    Output: the root node of the tree.\n",
    "    \"\"\"\n",
    "    root = None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    q = []\n",
    "    root = DecisionNode(data, chi=chi, max_depth=max_depth, gain_ratio=gain_ratio)\n",
    "    q.append(root)\n",
    "    while q:\n",
    "        n = q.pop(0)\n",
    "        n.split(impurity)\n",
    "        q.extend(n.children)\n",
    "    \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDq6pQ5S3e6g"
   },
   "outputs": [],
   "source": [
    "##### Your tests here #####\n",
    "\n",
    "tree_gini = build_tree(data=X_train, impurity=calc_gini) # gini and goodness of split\n",
    "tree_entropy = build_tree(data=X_train, impurity=calc_entropy) # entropy and goodness of split\n",
    "tree_entropy_gain_ratio = build_tree(data=X_train, impurity=calc_entropy, gain_ratio=True) # entropy and gain ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ED1C2e_d3e6g"
   },
   "source": [
    "## Tree evaluation (10 points) \n",
    "\n",
    "Implement the functions `predict` and `calc_accuracy` in `hw2.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oenFl1C03e6g"
   },
   "outputs": [],
   "source": [
    "#from hw2 import calc_accuracy, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpzG0CMB4IcW"
   },
   "outputs": [],
   "source": [
    "def predict(root, instance):\n",
    "    \"\"\"\n",
    "    Predict a given instance using the decision tree\n",
    " \n",
    "    Input:\n",
    "    - root: the root of the decision tree.\n",
    "    - instance: an row vector from the dataset. Note that the last element \n",
    "                of this vector is the label of the instance.\n",
    " \n",
    "    Output: the prediction of the instance.\n",
    "    \"\"\"\n",
    "    pred = None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    pred = None\n",
    "    curr_node = root\n",
    "    value_exist = True\n",
    "\n",
    "    while len(curr_node.children) > 0 and value_exist:\n",
    "        node_feature_index = curr_node.feature\n",
    "        instance_feature_value = instance[node_feature_index]\n",
    "\n",
    "        if instance_feature_value in curr_node.children_values:\n",
    "            next_child_index = curr_node.children_values.index(instance_feature_value)\n",
    "            curr_node = curr_node.children[next_child_index]\n",
    "        else:\n",
    "            value_exist = False\n",
    "\n",
    "    return curr_node.pred\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "\n",
    "def calc_accuracy(node, dataset):\n",
    "    \"\"\"\n",
    "    Predict a given dataset using the decision tree and calculate the accuracy\n",
    " \n",
    "    Input:\n",
    "    - node: a node in the decision tree.\n",
    "    - dataset: the dataset on which the accuracy is evaluated\n",
    " \n",
    "    Output: the accuracy of the decision tree on the given dataset (%).\n",
    "    \"\"\"\n",
    "    accuracy = 0\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    for i in dataset:\n",
    "        prediction = predict(node,i)\n",
    "        if prediction == i[-1]:\n",
    "            accuracy += 1\n",
    "    \n",
    "    accuracy = accuracy / len(dataset)\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQUwsQ3x3e6g"
   },
   "source": [
    "After building the three trees using the training set, you should calculate the accuracy on the test set. For each tree print the training and test accuracy. Select the tree that gave you the best test accuracy. For the rest of the exercise, use that tree (when you asked to build another tree use the same impurity function and same gain_ratio flag). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1cHRex1u3e6h",
    "outputId": "a1c8378b-5eec-4000-beff-b5bb4450d3bf"
   },
   "outputs": [],
   "source": [
    "##### Your tests here #####\n",
    "\n",
    "print('gini', calc_accuracy(tree_gini, X_train), calc_accuracy(tree_gini, X_test))\n",
    "print('entropy', calc_accuracy(tree_entropy, X_train), calc_accuracy(tree_entropy, X_test))\n",
    "print('entropy gain ratio', calc_accuracy(tree_entropy_gain_ratio, X_train), \n",
    "      calc_accuracy(tree_entropy_gain_ratio, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ro6kyDv03e6h"
   },
   "source": [
    "## Depth pruning (15 points)\n",
    "\n",
    "In this part, we will investigate the effect the max depth of the tree has on the training and testing accuracies.\n",
    "\n",
    "For each max_depth value in the range [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], construct a tree and prune it according to the max_depth value (don't let the tree to grow beyond this depth). Next, calculate the training and testing accuracy on the resulting trees. \n",
    "\n",
    "In order to debug and self-test your code, draw the training and testing accuracy as a function of the max_depth and verify that your results make sense. The red dot denotes the best model according to the testing accuracy.\n",
    "\n",
    "Implement the function `depth_pruning` in `hw2.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmZuAVTx7yvX"
   },
   "outputs": [],
   "source": [
    "def depth_pruning(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Calculate the training and testing accuracies for different depths\n",
    "    using the best impurity function and the gain_ratio flag you got\n",
    "    previously.\n",
    "\n",
    "    Input:\n",
    "    - X_train: the training data where the last column holds the labels\n",
    "    - X_test: the testing data where the last column holds the labels\n",
    " \n",
    "    Output: the training and testing accuracies per max depth\n",
    "    \"\"\"\n",
    "    training = []\n",
    "    testing  = []\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    for max_depth in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "        tree_entropy_gain_ratio = build_tree(data=X_train, impurity=calc_entropy, gain_ratio=True, max_depth=max_depth) # entropy and gain ratio\n",
    "        train_acc = calc_accuracy(tree_entropy_gain_ratio, X_train)\n",
    "        test_acc = calc_accuracy(tree_entropy_gain_ratio, X_test)\n",
    "        training.append(train_acc)\n",
    "        testing.append(test_acc)\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return training, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "id": "_wEu-2Go3e6h",
    "outputId": "3f7b1ecb-95cd-404c-c8ec-4d804984bcb7"
   },
   "outputs": [],
   "source": [
    "##### Your tests here #####\n",
    "#from hw2 import depth_pruning\n",
    "depth_training_acc, depth_testing_acc = depth_pruning(X_train, X_test)\n",
    "\n",
    "plt.plot(range(1, 11), depth_training_acc, label='Training')\n",
    "plt.plot(range(1, 11), depth_testing_acc, label='Test')\n",
    "plt.scatter(np.argmax(depth_testing_acc)+1, max(depth_testing_acc), c='r')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "An4JacFL3e6i"
   },
   "source": [
    "## Chi square pre-pruning (15 points)\n",
    "\n",
    "Consider the following p-value cut-off values: [1 (no pruning), 0.5, 0.25, 0.1, 0.05, 0.0001 (max pruning)]. For each value, construct a tree and prune it according to the cut-off value. Next, calculate the training and testing accuracy on the resulting trees. \n",
    "\n",
    "In order to debug and self-test your code, draw the training and testing accuracy as a function of the tuple (p-value, tree depth) and verify that your results make sense. The red dot denotes the best model according to the testing accuracy.\n",
    "\n",
    "Implement the function `chi_pruning` in `hw2.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5GUySLO3e6i"
   },
   "outputs": [],
   "source": [
    "#from hw2 import chi_pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RkWxM6AY84-b"
   },
   "outputs": [],
   "source": [
    "def calc_tree_depth(root):\n",
    "    l = [root]\n",
    "    depth = 0\n",
    "    while l:\n",
    "        depth = depth + 1\n",
    "        new_l = []\n",
    "        while l:\n",
    "            n = l.pop(0)\n",
    "            new_l.extend(n.children)\n",
    "        l = new_l\n",
    "    return depth\n",
    "\n",
    "\n",
    "def chi_pruning(X_train, X_test):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the training and testing accuracies for different chi values\n",
    "    using the best impurity function and the gain_ratio flag you got\n",
    "    previously. \n",
    "\n",
    "    Input:\n",
    "    - X_train: the training data where the last column holds the labels\n",
    "    - X_test: the testing data where the last column holds the labels\n",
    " \n",
    "    Output:\n",
    "    - chi_training_acc: the training accuracy per chi value\n",
    "    - chi_testing_acc: the testing accuracy per chi value\n",
    "    - depths: the tree depth for each chi value\n",
    "    \"\"\"\n",
    "    chi_training_acc = []\n",
    "    chi_testing_acc  = []\n",
    "    depth = []\n",
    "    for chi in [1, 0.5, 0.25, 0.1, 0.05, 0.0001]:\n",
    "        tree_entropy_gain_ratio = build_tree(data=X_train, impurity=calc_entropy, gain_ratio=True, max_depth=1000, chi=chi) # entropy and gain ratio\n",
    "        train_acc = calc_accuracy(tree_entropy_gain_ratio, X_train)\n",
    "        test_acc = calc_accuracy(tree_entropy_gain_ratio, X_test)\n",
    "        depth.append(calc_tree_depth(tree_entropy_gain_ratio))\n",
    "        chi_training_acc.append(train_acc)\n",
    "        chi_testing_acc.append(test_acc)\n",
    "    return chi_training_acc, chi_testing_acc, depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "id": "7H0e6pJI3e6i",
    "outputId": "ae5ce58d-1388-482c-d9db-f0251273feba"
   },
   "outputs": [],
   "source": [
    "##### Your tests here #####\n",
    "\n",
    "chi_training_acc, chi_testing_acc, depth = chi_pruning(X_train, X_test)\n",
    "\n",
    "chi_depth_tuple = [str((x, y)) for x, y in zip([1, 0.5, 0.25, 0.1, 0.05, 0.0001], depth)][::-1]\n",
    "plt.plot(chi_depth_tuple, chi_training_acc[::-1], label='Training')\n",
    "plt.plot(chi_depth_tuple, chi_testing_acc[::-1], label='Test')\n",
    "plt.scatter(chi_depth_tuple[np.argmax(chi_testing_acc[::-1])], max(chi_testing_acc), c='r')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU_3cOdD3e6j"
   },
   "source": [
    "Build the best 2 trees:\n",
    "1. tree_max_depth - the best tree according to max_depth pruning\n",
    "1. tree_chi - the best tree according to chi square pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KrglK2xW3e6j"
   },
   "outputs": [],
   "source": [
    "tree_max_depth = build_tree(data=X_train, impurity=calc_entropy, gain_ratio=True)\n",
    "tree_chi = build_tree(data=X_train, impurity=calc_entropy, gain_ratio=True,chi=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBkpDRJa3e6k"
   },
   "source": [
    "## Number of Nodes (5 points) \n",
    "\n",
    "Of the two trees above we will choose the one with fewer nodes.\n",
    "\n",
    "Complete the function counts_nodes and print the number of nodes in each tree\n",
    "\n",
    "Implement the function `count_nodes` in `hw2.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmgYP42h3e6k"
   },
   "outputs": [],
   "source": [
    "#from hw2 import count_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIGvYuTB9Qjg"
   },
   "outputs": [],
   "source": [
    "def count_nodes(node):\n",
    "    \"\"\"\n",
    "    Count the number of node in a given tree\n",
    " \n",
    "    Input:\n",
    "    - node: a node in the decision tree.\n",
    " \n",
    "    Output: the number of nodes in the tree.\n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    count = 1\n",
    "    for child in node.children:\n",
    "        count += count_nodes(child)\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w_7F7rsZ3e6k",
    "outputId": "ce936f80-232f-480a-c04f-1433adcafc4f"
   },
   "outputs": [],
   "source": [
    "##### Your tests here #####\n",
    "\n",
    "print(count_nodes(tree_max_depth))\n",
    "print(count_nodes(tree_chi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBs1RGwf3e6k"
   },
   "source": [
    "## Print the tree\n",
    "\n",
    "We provided you with a function that should print your tree for your own debugging purposes. \n",
    "\n",
    "This code prints:\n",
    "```\n",
    "[ROOT, feature=X0],\n",
    "  [X0=a, feature=X2]\n",
    "    [X2=c, leaf]: [{1.0: 10}]\n",
    "    [X2=d, leaf]: [{0.0: 10}]\n",
    "  [X0=y, feature=X5], \n",
    "       [X5=a, leaf]: [{1.0: 5}]\n",
    "       [X5=s, leaf]: [{0.0: 10}]\n",
    "  [X0=e, leaf]: [{0.0: 25, 1.0: 50}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJsjlZJl3e6l"
   },
   "outputs": [],
   "source": [
    "def print_tree(node, depth=0, parent_feature='ROOT', feature_val='ROOT'):\n",
    "    '''\n",
    "    prints the tree according to the example above\n",
    "\n",
    "    Input:\n",
    "    - node: a node in the decision tree\n",
    "\n",
    "    This function has no return value\n",
    "    '''\n",
    "    if node.terminal == False:\n",
    "        if node.depth == 0:\n",
    "            print('[ROOT, feature=X{}]'.format(node.feature))\n",
    "        else:\n",
    "            print('{}[X{}={}, feature=X{}], Depth: {}'.format(depth*'  ', parent_feature, feature_val, \n",
    "                                                              node.feature, node.depth))\n",
    "        for i, child in enumerate(node.children):\n",
    "            print_tree(child, depth+1, node.feature, node.children_values[i])\n",
    "    else:\n",
    "        classes_count = {}\n",
    "        labels, counts = np.unique(node.data[:, -1], return_counts=True)\n",
    "        for l, c in zip(labels, counts):\n",
    "            classes_count[l] = c\n",
    "        print('{}[X{}={}, leaf]: [{}], Depth: {}'.format(depth*'  ', parent_feature, feature_val,\n",
    "                                                         classes_count, node.depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ZTLySSL3e6l",
    "outputId": "91e37e75-a627-463e-b32c-e9c61ba81a59"
   },
   "outputs": [],
   "source": [
    "print_tree(tree_max_depth)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
